{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nfrom string import Template # For generating prompt template\n\nimport os\nimport gc # grabage collector\n# we need to install the sentence transformer and use its embedding to read the faiss index\n#cp stands for a copy. This command is used to copy files or groups of files or directories. \n# The -r option tells rm to remove directories recursively, and the -f option tells it to force the removal of files and directories that are read-only or do not exist\n\n!cp -rf /kaggle/input/sentence-transformers-222/sentence-transformers /kaggle/working/sentence-transformers\n!pip install -U /kaggle/working/sentence-transformers\n\n#installing faiss package for reading faiss wikipedia index\n!pip install -U /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n# as per wikipedia faiss index https://www.kaggle.com/datasets/jjinho/wikipedia-2023-07-faiss-index\nimport faiss\nfrom faiss import write_index, read_index\n\n\nimport ctypes\nlibc = ctypes.CDLL(\"libc.so.6\")\n\n# installing langchain package# We will use langchain recursive splitter\n!pip install langchain --no-index --find-links=file:///kaggle/input/llm-pkg/\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n\n\nfrom tqdm.auto import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-09-18T09:31:24.430104Z","iopub.execute_input":"2023-09-18T09:31:24.430895Z","iopub.status.idle":"2023-09-18T09:33:10.448293Z","shell.execute_reply.started":"2023-09-18T09:31:24.430859Z","shell.execute_reply":"2023-09-18T09:33:10.447139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n# Reading the csv file\n#df_train = pd.read_csv(\"./train.csv\")\n#df_train = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/train.csv\")\n\n#df_train = pd.read_csv(\"/kaggle/input/additional-train-data-for-llm-science-exam/6000_train_examples.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\")\n#df_extra = pd.read_csv('/kaggle/input/additional-train-data-for-llm-science-exam/extra_train_set.csv')\n#df_train.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T09:33:10.450679Z","iopub.execute_input":"2023-09-18T09:33:10.452613Z","iopub.status.idle":"2023-09-18T09:33:10.550442Z","shell.execute_reply.started":"2023-09-18T09:33:10.452573Z","shell.execute_reply":"2023-09-18T09:33:10.549469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_train.reset_index(inplace=True)\n# df_train.rename(columns={'index':'id'},inplace=True)\n# df_train = df_train\n","metadata":{"execution":{"iopub.status.busy":"2023-09-18T09:33:10.552000Z","iopub.execute_input":"2023-09-18T09:33:10.552349Z","iopub.status.idle":"2023-09-18T09:33:10.566151Z","shell.execute_reply.started":"2023-09-18T09:33:10.552315Z","shell.execute_reply":"2023-09-18T09:33:10.565217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## READING WIKIPEDIA FILES TO FIND CONTEXT","metadata":{}},{"cell_type":"code","source":"# PART 1 - Searching Wikipedia Titles","metadata":{"execution":{"iopub.status.busy":"2023-09-18T09:33:10.569338Z","iopub.execute_input":"2023-09-18T09:33:10.570113Z","iopub.status.idle":"2023-09-18T09:33:10.577139Z","shell.execute_reply.started":"2023-09-18T09:33:10.570079Z","shell.execute_reply":"2023-09-18T09:33:10.576213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loadding the wikipedia faiss index. This will be used for searching\nsentence_index = read_index(\"/kaggle/input/wikipedia-2023-07-faiss-index/wikipedia_202307.index\")","metadata":{"execution":{"iopub.status.busy":"2023-09-18T09:33:10.578625Z","iopub.execute_input":"2023-09-18T09:33:10.578963Z","iopub.status.idle":"2023-09-18T09:34:40.131920Z","shell.execute_reply.started":"2023-09-18T09:33:10.578921Z","shell.execute_reply":"2023-09-18T09:34:40.130520Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating index of prompts i.e q to search for relavnt wikipedia documents\nfrom sentence_transformers import SentenceTransformer\nSIM_MODEL = '/kaggle/input/sentencetransformers-allminilml6v2/sentence-transformers_all-MiniLM-L6-v2'\nDEVICE = 0\nMAX_LENGTH = 384\nBATCH_SIZE = 16\n\nmodel = SentenceTransformer(SIM_MODEL, device='cuda')\nmodel.max_seq_length = MAX_LENGTH\nmodel = model.half() # The model.half() method in PyTorch is used to convert a model to half-precision. This can be useful for reducing the memory footprint of a model, as half-precision numbers use half the memory as single-precision numbers","metadata":{"execution":{"iopub.status.busy":"2023-09-18T09:34:40.134805Z","iopub.execute_input":"2023-09-18T09:34:40.135205Z","iopub.status.idle":"2023-09-18T09:34:43.566569Z","shell.execute_reply.started":"2023-09-18T09:34:40.135170Z","shell.execute_reply":"2023-09-18T09:34:43.565503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prompt_embeddings_train = model.encode(df_train['prompt'].values, batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\n# prompt_embeddings_train = prompt_embeddings_train.detach().cpu().numpy() # detach to remove gradients.\n# search_score_train, search_index_train = sentence_index.search(prompt_embeddings_train, 5)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T09:34:43.567976Z","iopub.execute_input":"2023-09-18T09:34:43.568372Z","iopub.status.idle":"2023-09-18T09:39:23.360719Z","shell.execute_reply.started":"2023-09-18T09:34:43.568335Z","shell.execute_reply":"2023-09-18T09:39:23.359648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# del prompt_embeddings_train\n# _ = gc.collect() # garbage collector..frees up memmory\n# libc.malloc_trim(0)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T09:39:23.362417Z","iopub.execute_input":"2023-09-18T09:39:23.362789Z","iopub.status.idle":"2023-09-18T09:39:23.692679Z","shell.execute_reply.started":"2023-09-18T09:39:23.362753Z","shell.execute_reply":"2023-09-18T09:39:23.691670Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt_embeddings_test = model.encode(df_test['prompt'].values, batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\nprompt_embeddings_test = prompt_embeddings_test.detach().cpu().numpy() # detach to remove gradients.\nsearch_score_test, search_index_test = sentence_index.search(prompt_embeddings_test, 5)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T09:39:23.694336Z","iopub.execute_input":"2023-09-18T09:39:23.694759Z","iopub.status.idle":"2023-09-18T09:39:37.384891Z","shell.execute_reply.started":"2023-09-18T09:39:23.694725Z","shell.execute_reply":"2023-09-18T09:39:37.383744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndel sentence_index # deleting as not required. otherwise it will give memory issue\n\ndel prompt_embeddings_test\n_ = gc.collect() # garbage collector..frees up memmory\nlibc.malloc_trim(0)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T09:39:37.389229Z","iopub.execute_input":"2023-09-18T09:39:37.389538Z","iopub.status.idle":"2023-09-18T09:39:38.271858Z","shell.execute_reply.started":"2023-09-18T09:39:37.389511Z","shell.execute_reply":"2023-09-18T09:39:38.270969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T09:39:38.273467Z","iopub.execute_input":"2023-09-18T09:39:38.273841Z","iopub.status.idle":"2023-09-18T09:39:38.281689Z","shell.execute_reply.started":"2023-09-18T09:39:38.273808Z","shell.execute_reply":"2023-09-18T09:39:38.280538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PART 2 - Fetching relavant text of wikipedia documents","metadata":{"execution":{"iopub.status.busy":"2023-09-18T09:39:38.283548Z","iopub.execute_input":"2023-09-18T09:39:38.284045Z","iopub.status.idle":"2023-09-18T09:39:38.293349Z","shell.execute_reply.started":"2023-09-18T09:39:38.284006Z","shell.execute_reply":"2023-09-18T09:39:38.292389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n# getting wikipedia documents \ndef wiki_context(search_score,search_index):\n    df_wiki = pd.read_parquet(\"/kaggle/input/wikipedia-20230701/wiki_2023_index.parquet\",\n                         columns=['id', 'file'])\n    wikipedia_file_data = []\n\n    for i, (scr, idx) in tqdm(enumerate(zip(search_score, search_index)), total=len(search_score)):\n        scr_idx = idx\n        _df = df_wiki.loc[scr_idx].copy()\n        _df['prompt_id'] = i\n        wikipedia_file_data.append(_df)\n    wikipedia_file_data = pd.concat(wikipedia_file_data).reset_index(drop=True)\n    wikipedia_file_data = wikipedia_file_data[['id', 'prompt_id', 'file']].drop_duplicates().sort_values(['file', 'id']).reset_index(drop=True)\n\n    WIKI_PATH = \"/kaggle/input/wikipedia-20230701\"\n    wiki_files = os.listdir(WIKI_PATH)\n\n    wiki_text_data = []\n\n    for file in tqdm(wikipedia_file_data.file.unique(), total=len(wikipedia_file_data.file.unique())):\n        _id = [str(i) for i in wikipedia_file_data[wikipedia_file_data['file']==file]['id'].tolist()]\n        _df = pd.read_parquet(f\"{WIKI_PATH}/{file}\", columns=['id', 'text'])\n\n        _df_temp = _df[_df['id'].isin(_id)].copy()\n        del _df\n        _ = gc.collect()\n        libc.malloc_trim(0)\n        wiki_text_data.append(_df_temp)\n    wiki_text_data = pd.concat(wiki_text_data).drop_duplicates().reset_index(drop=True)\n    del df_wiki\n    _ = gc.collect()\n    libc.malloc_trim(0)\n    context_df = wikipedia_file_data.merge(wiki_text_data,on='id')\n    return context_df","metadata":{"execution":{"iopub.status.busy":"2023-09-18T09:39:38.294949Z","iopub.execute_input":"2023-09-18T09:39:38.295518Z","iopub.status.idle":"2023-09-18T09:39:38.309472Z","shell.execute_reply.started":"2023-09-18T09:39:38.295482Z","shell.execute_reply":"2023-09-18T09:39:38.308345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# del df_wiki\n# _ = gc.collect()\n# libc.malloc_trim(0)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T09:39:38.310969Z","iopub.execute_input":"2023-09-18T09:39:38.311397Z","iopub.status.idle":"2023-09-18T09:39:38.325336Z","shell.execute_reply.started":"2023-09-18T09:39:38.311336Z","shell.execute_reply":"2023-09-18T09:39:38.324014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"context_df_train = wiki_context(search_score_train,search_index_train)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-18T09:39:38.327127Z","iopub.execute_input":"2023-09-18T09:39:38.327686Z","iopub.status.idle":"2023-09-18T09:44:23.247202Z","shell.execute_reply.started":"2023-09-18T09:39:38.327651Z","shell.execute_reply":"2023-09-18T09:44:23.246036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"context_df_test = wiki_context(search_score_test,search_index_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T09:44:23.248816Z","iopub.execute_input":"2023-09-18T09:44:23.249207Z","iopub.status.idle":"2023-09-18T09:49:00.491510Z","shell.execute_reply.started":"2023-09-18T09:44:23.249173Z","shell.execute_reply":"2023-09-18T09:49:00.488033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Spliting the wiki text in the context df in chunk size\n\nchunk_size = 2400\nchunk_overlap = 400\n\nr_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=chunk_size,\n    chunk_overlap=chunk_overlap\n)\n\nsplit_text =[]\nfor i in range(len(context_df_train)):\n    split_text.append ( r_splitter.split_text(context_df_train.loc[i,'text']))\ncontext_df_train['split'] = split_text\n","metadata":{"execution":{"iopub.status.busy":"2023-09-18T09:49:00.492955Z","iopub.execute_input":"2023-09-18T09:49:00.493350Z","iopub.status.idle":"2023-09-18T09:50:15.129957Z","shell.execute_reply.started":"2023-09-18T09:49:00.493312Z","shell.execute_reply":"2023-09-18T09:50:15.128945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\ndef format_dataframe(df, context_df):\n    \n    model = SentenceTransformer(SIM_MODEL, device='cuda')\n    model.max_seq_length = 384\n    model = model.half() # The model.half() method in PyTorch is used to convert a model to half-precision. This can be useful for reducing the memory footprint of a model, as half-precision numbers use half the memory as single-precision numbers\n    \n #   final_prompt = []\n    #for i in range(5):\n    for i in range(len(df)):\n        q = df.iloc[i]['prompt']\n        idx = df.iloc[i]['id']\n        chunk = ''\n        \n        text_rel = context_df[context_df['prompt_id'] == idx].iloc[:]['split']\n        text = []\n        for j in range(len(text_rel)):\n            text.extend(text_rel.iloc[j])\n        if text != []:\n            text_df = pd.DataFrame(text,columns=['text'])\n            vectors = model.encode(text_df['text'])\n            vector_dimension = vectors.shape[1]\n            index = faiss.IndexFlatL2(vector_dimension)\n            faiss.normalize_L2(vectors)\n            index.add(vectors)\n\n\n            search_vector = model.encode(q)\n            _vector = np.array([search_vector])\n            faiss.normalize_L2(_vector)\n\n            k = 1\n            distances, ann = index.search(_vector, k=k)\n            chunk = text[ann[0,0]]\n            chunk = re.sub('[^a-zA-Z0-9 \\n\\.]', '', chunk)\n\n        df.iloc[i,8] = str(chunk)\n    \n    del index\n    del search_vector\n    del model\n    _ = gc.collect()\n    libc.malloc_trim(0)\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2023-09-18T09:50:15.131513Z","iopub.execute_input":"2023-09-18T09:50:15.131871Z","iopub.status.idle":"2023-09-18T09:50:15.144896Z","shell.execute_reply.started":"2023-09-18T09:50:15.131838Z","shell.execute_reply":"2023-09-18T09:50:15.143999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_train['context'] = ''","metadata":{"execution":{"iopub.status.busy":"2023-09-18T09:50:15.146932Z","iopub.execute_input":"2023-09-18T09:50:15.147839Z","iopub.status.idle":"2023-09-18T09:50:15.153316Z","shell.execute_reply.started":"2023-09-18T09:50:15.147807Z","shell.execute_reply":"2023-09-18T09:50:15.152468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_train_df = format_dataframe(df_train,context_df_train)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-09-18T09:50:15.154784Z","iopub.execute_input":"2023-09-18T09:50:15.155150Z","iopub.status.idle":"2023-09-18T10:02:59.780864Z","shell.execute_reply.started":"2023-09-18T09:50:15.155118Z","shell.execute_reply":"2023-09-18T10:02:59.779827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_train_df.to_csv(\"context_6k_2400_400.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-09-18T10:02:59.783227Z","iopub.execute_input":"2023-09-18T10:02:59.783634Z","iopub.status.idle":"2023-09-18T10:03:00.314445Z","shell.execute_reply.started":"2023-09-18T10:02:59.783597Z","shell.execute_reply":"2023-09-18T10:03:00.313184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TRAINING THE MODEL","metadata":{}},{"cell_type":"code","source":"\nimport torch, gc\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T10:04:09.463688Z","iopub.execute_input":"2023-09-18T10:04:09.464096Z","iopub.status.idle":"2023-09-18T10:04:10.861003Z","shell.execute_reply.started":"2023-09-18T10:04:09.464062Z","shell.execute_reply":"2023-09-18T10:04:10.860004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model_train[[\"prompt\", \"context\", \"A\", \"B\", \"C\", \"D\", \"E\",\"answer\"]].to_csv(\"./train_context.csv\", index=False)\n#model_train_df = pd.read_csv(\"train_context.csv\")\n#model_train_df.index = list(range(len(model_train_df)))\n#model_train_df['id'] = list(range(len(model_train_df)))\n#model_train_df['context'] = model_train_df['context'].apply(lambda x: str(x))\n#model_train_df[\"prompt\"] = model_train_df[\"context\"] + \" #### \" +  model_train_df[\"prompt\"]\n#model_train_df['answer'] = 'B'","metadata":{"execution":{"iopub.status.busy":"2023-09-18T10:04:13.778804Z","iopub.execute_input":"2023-09-18T10:04:13.779954Z","iopub.status.idle":"2023-09-18T10:04:13.785343Z","shell.execute_reply.started":"2023-09-18T10:04:13.779908Z","shell.execute_reply":"2023-09-18T10:04:13.784034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n\nfrom typing import Optional, Union\nimport pandas as pd, numpy as np, torch\nfrom datasets import Dataset\nfrom dataclasses import dataclass\nfrom transformers import AutoTokenizer\nfrom transformers import EarlyStoppingCallback\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\nfrom transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n\n\nVER=10\n# TRAIN WITH SUBSET OF 60K\nNUM_TRAIN_SAMPLES = 2048#1_024\n# PARAMETER EFFICIENT FINE TUNING\n# PEFT REQUIRES 1XP100 GPU NOT 2XT4\nUSE_PEFT = False\n# NUMBER OF LAYERS TO FREEZE \n# DEBERTA LARGE HAS TOTAL OF 24 LAYERS\nFREEZE_LAYERS = 18#18\n# BOOLEAN TO FREEZE EMBEDDINGS\nFREEZE_EMBEDDINGS = True\n# LENGTH OF CONTEXT PLUS QUESTION ANSWER\nMAX_INPUT = 256\n# HUGGING FACE MODEL\nMODEL = \"/kaggle/input/debertav3model/LLMQAModel\"#'microsoft/deberta-v3-large'\nmodel = AutoModelForMultipleChoice.from_pretrained(MODEL)\ntokenizer = AutoTokenizer.from_pretrained(MODEL)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T10:04:58.828937Z","iopub.execute_input":"2023-09-18T10:04:58.829354Z","iopub.status.idle":"2023-09-18T10:05:04.649960Z","shell.execute_reply.started":"2023-09-18T10:04:58.829322Z","shell.execute_reply":"2023-09-18T10:05:04.648889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_examples = len(model_train_df)\nsplit_size = 0.65*total_examples\nsplit_size = int(split_size)\ntrain_ds = Dataset.from_pandas(model_train_df[:split_size])\neval_ds = Dataset.from_pandas(model_train_df[split_size:])","metadata":{"execution":{"iopub.status.busy":"2023-09-18T10:05:04.659589Z","iopub.execute_input":"2023-09-18T10:05:04.659968Z","iopub.status.idle":"2023-09-18T10:05:04.715524Z","shell.execute_reply.started":"2023-09-18T10:05:04.659935Z","shell.execute_reply":"2023-09-18T10:05:04.714580Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"option_to_index = {option: idx for idx, option in enumerate('ABCDE')}\nindex_to_option = {v: k for k,v in option_to_index.items()}\n\ndef preprocess(example):\n    first_sentence = [ \"[CLS] \" + example['context'] ] * 5\n    second_sentences = [\" #### \" + str(example['prompt']) + \" [SEP] \" + str(example[option]) + \" [SEP]\" for option in 'ABCDE']\n    tokenized_example = tokenizer(first_sentence, second_sentences, truncation='only_first', \n                                  max_length=MAX_INPUT,padding='max_length', add_special_tokens=False)\n    tokenized_example['label'] = option_to_index[example['answer']]\n    \n    return tokenized_example\n\n@dataclass\nclass DataCollatorForMultipleChoice:\n    tokenizer: PreTrainedTokenizerBase\n    padding: Union[bool, str, PaddingStrategy] = True\n    max_length: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n    \n    def __call__(self, features):\n        label_name = 'label' if 'label' in features[0].keys() else 'labels'\n        labels = [feature.pop(label_name) for feature in features]\n        batch_size = len(features)\n        num_choices = len(features[0]['input_ids'])\n        flattened_features = [\n            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n        ]\n        flattened_features = sum(flattened_features, [])\n        \n        batch = self.tokenizer.pad(\n            flattened_features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors='pt',\n        )\n        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n        return batch","metadata":{"execution":{"iopub.status.busy":"2023-09-18T10:05:18.416829Z","iopub.execute_input":"2023-09-18T10:05:18.417220Z","iopub.status.idle":"2023-09-18T10:05:18.431118Z","shell.execute_reply.started":"2023-09-18T10:05:18.417188Z","shell.execute_reply":"2023-09-18T10:05:18.429871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_train_ds = train_ds.map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer','context'])\ntokenized_eval_ds = eval_ds.map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer','context'])","metadata":{"execution":{"iopub.status.busy":"2023-09-18T10:05:20.579773Z","iopub.execute_input":"2023-09-18T10:05:20.580385Z","iopub.status.idle":"2023-09-18T10:06:04.870846Z","shell.execute_reply.started":"2023-09-18T10:05:20.580350Z","shell.execute_reply":"2023-09-18T10:06:04.869767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# NOTE PEFT REQUIRES US TO USE 1XP100 NOT 2XT4. I'M NOT SURE WHY.\nif USE_PEFT:\n    !pip install --no-index --no-deps /kaggle/input/llm-whls/peft-0.4.0-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2023-09-18T10:06:08.407887Z","iopub.execute_input":"2023-09-18T10:06:08.408310Z","iopub.status.idle":"2023-09-18T10:06:08.414434Z","shell.execute_reply.started":"2023-09-18T10:06:08.408277Z","shell.execute_reply":"2023-09-18T10:06:08.413300Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if USE_PEFT:\n    print('We are using PEFT.')\n    from peft import LoraConfig, get_peft_model, TaskType\n    peft_config = LoraConfig(\n        r=8, lora_alpha=4, task_type=TaskType.SEQ_CLS, lora_dropout=0.2, \n        bias=\"none\", inference_mode=False, \n        target_modules=[\"query_proj\", \"value_proj\"],\n        modules_to_save=['classifier','pooler'],\n    )\n    model = get_peft_model(model, peft_config)\n    model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T10:06:10.752753Z","iopub.execute_input":"2023-09-18T10:06:10.753174Z","iopub.status.idle":"2023-09-18T10:06:10.759885Z","shell.execute_reply.started":"2023-09-18T10:06:10.753141Z","shell.execute_reply":"2023-09-18T10:06:10.758548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if FREEZE_EMBEDDINGS:\n    print('Freezing embeddings.')\n    for param in model.deberta.embeddings.parameters():\n        param.requires_grad = False\nif FREEZE_LAYERS>0:\n    print(f'Freezing {FREEZE_LAYERS} layers.')\n    for layer in model.deberta.encoder.layer[:FREEZE_LAYERS]:\n        for param in layer.parameters():\n            param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2023-09-18T10:06:13.073166Z","iopub.execute_input":"2023-09-18T10:06:13.073547Z","iopub.status.idle":"2023-09-18T10:06:13.082460Z","shell.execute_reply.started":"2023-09-18T10:06:13.073516Z","shell.execute_reply":"2023-09-18T10:06:13.081245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def map_at_3(predictions, labels):\n    map_sum = 0\n    pred = np.argsort(-1*np.array(predictions),axis=1)[:,:3]\n    for x,y in zip(pred,labels):\n        z = [1/i if y==j else 0 for i,j in zip([1,2,3],x)]\n        map_sum += np.sum(z)\n    return map_sum / len(predictions)\n\ndef compute_metrics(p):\n    predictions = p.predictions.tolist()\n    labels = p.label_ids.tolist()\n    return {\"map@3\": map_at_3(predictions, labels)}\n","metadata":{"execution":{"iopub.status.busy":"2023-09-18T10:06:14.729662Z","iopub.execute_input":"2023-09-18T10:06:14.730056Z","iopub.status.idle":"2023-09-18T10:06:14.739676Z","shell.execute_reply.started":"2023-09-18T10:06:14.730021Z","shell.execute_reply":"2023-09-18T10:06:14.738663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    warmup_ratio=0.1, \n    learning_rate=2e-5,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=4,\n    num_train_epochs=5,\n    report_to='none',\n    output_dir = f'./checkpoints_{VER}',\n    overwrite_output_dir=True,\n    fp16=True,\n    gradient_accumulation_steps=8,#8\n    logging_steps=50,\n    evaluation_strategy='steps',\n    eval_steps=50,\n    save_strategy=\"steps\",\n    save_steps=50,\n    load_best_model_at_end=True,\n    metric_for_best_model='map@3',\n    lr_scheduler_type='cosine',\n    weight_decay=0.01,\n    save_total_limit=4,\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T10:06:25.198962Z","iopub.execute_input":"2023-09-18T10:06:25.199474Z","iopub.status.idle":"2023-09-18T10:06:25.213758Z","shell.execute_reply.started":"2023-09-18T10:06:25.199424Z","shell.execute_reply":"2023-09-18T10:06:25.212795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    tokenizer=tokenizer,\n    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n    train_dataset=tokenized_train_ds,\n    eval_dataset=tokenized_eval_ds,\n    compute_metrics = compute_metrics,\n    #callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],\n)\n\ntrainer.train()\ntrainer.save_model(f'model_v{VER}')","metadata":{"execution":{"iopub.status.busy":"2023-09-18T10:06:46.998291Z","iopub.execute_input":"2023-09-18T10:06:46.998724Z","iopub.status.idle":"2023-09-18T14:01:49.008113Z","shell.execute_reply.started":"2023-09-18T10:06:46.998692Z","shell.execute_reply":"2023-09-18T14:01:49.006963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del model, trainer\nif USE_PEFT:\n    model = AutoModelForMultipleChoice.from_pretrained(MODEL)\n    model = get_peft_model(model, peft_config)\n    checkpoint = torch.load(f'model_v{VER}/pytorch_model.bin')\n    model.load_state_dict(checkpoint)\nelse:\n    model = AutoModelForMultipleChoice.from_pretrained(f'model_v{VER}')\ntrainer = Trainer(model=model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.kaggle.com/code/philippsinger/h2ogpt-perplexity-ranking\nimport numpy as np\ndef precision_at_k(r, k):\n    \"\"\"Precision at k\"\"\"\n    assert k <= len(r)\n    assert k != 0\n    return sum(int(x) for x in r[:k]) / k\n\ndef MAP_at_3(predictions, true_items):\n    \"\"\"Score is mean average precision at 3\"\"\"\n    U = len(predictions)\n    map_at_3 = 0.0\n    for u in range(U):\n        user_preds = predictions[u].split()\n        user_true = true_items[u]\n        user_results = [1 if item == user_true else 0 for item in user_preds]\n        for k in range(min(len(user_preds), 3)):\n            map_at_3 += precision_at_k(user_results, k+1) * user_results[k]\n    return map_at_3 / U","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_train_df.to_csv(\"context_train.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## INFERENCING ON TEST DF","metadata":{}},{"cell_type":"code","source":"del df_train\ndel context_df_train\ndel model_train_df\ndel tokenized_train_ds\ndel tokenized_eval_ds\n\n_ = gc.collect() # garbage collector..frees up memmory\nlibc.malloc_trim(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chunk_size = 1200\nchunk_overlap = 200\n\nr_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=chunk_size,\n    chunk_overlap=chunk_overlap\n)\n\nsplit_text =[]\nfor i in range(len(context_df_test)):\n    split_text.append ( r_splitter.split_text(context_df_test.loc[i,'text']))\ncontext_df_test['split'] = split_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\")\ndf_test['answer'] = 'B'\ndf_test['context']= ' '\ntest_df = format_dataframe(df_test,context_df_test)","metadata":{"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model_test[[\"prompt\", \"context\", \"A\", \"B\", \"C\", \"D\", \"E\"]].to_csv(\"./test_context.csv\", index=False)\n#test_df = pd.read_csv(\"test_context.csv\")\n#test_df.index = list(range(len(test_df)))\n#test_df['id'] = list(range(len(test_df)))\n#test_df['context'] = test_df['context'].apply(lambda x: str(x))\n#test_df[\"prompt\"] = test_df[\"context\"] + \" #### \" +  test_df[\"prompt\"]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds = Dataset.from_pandas(test_df)\ntokenized_test_ds = test_ds.map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer','context'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntest_predictions = trainer.predict(tokenized_test_ds).predictions\npredictions_as_ids = np.argsort(-test_predictions, 1)\npredictions_as_answer_letters = np.array(list('ABCDE'))[predictions_as_ids]\npredictions_as_string = test_df['prediction'] = [\n    ' '.join(row) for row in predictions_as_answer_letters[:, :3]\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m = MAP_at_3(test_df.prediction.values, test_df.answer.values)\nprint( 'CV MAP@3 =',m )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The following function gets the indices of the highest scoring answers for each row\n# and converts them back to our answer format (A, B, C, D, E)\nimport numpy as np\ndef predictions_to_map_output(predictions):\n    sorted_answer_indices = np.argsort(-predictions)\n    top_answer_indices = sorted_answer_indices[:,:3] # Get the first three answers in each row\n    top_answers = np.vectorize(index_to_option.get)(top_answer_indices)\n    return np.apply_along_axis(lambda row: ' '.join(row), 1, top_answers)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = test_df[['id']]\nsubmission_df['prediction'] = predictions_to_map_output(test_predictions)\n\nsubmission_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Once we write our submission file we're good to submit!\nif os.path.exists('submission.csv'):\n    os.remove('submission.csv')\nsubmission_df.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nimport gc\nimport os\nimport re\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport torch\nfrom torch import nn\nfrom transformers import LlamaTokenizer, AutoModelForCausalLM","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\")\n#sub = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/sample_submission.csv\")\n\ndf['instruction'] = df['prompt'] + ' A: ' + df['A'] + ' B: ' + df['B'] + ' C: ' + df['C'] + ' D: ' + df['D'] + ' E: ' + df['E']\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:27:54.414106Z","iopub.execute_input":"2023-09-21T15:27:54.416619Z","iopub.status.idle":"2023-09-21T15:27:54.453098Z","shell.execute_reply.started":"2023-09-21T15:27:54.416583Z","shell.execute_reply":"2023-09-21T15:27:54.452186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = '/kaggle/input/llama2-7b-stem'\n\ntokenizer = LlamaTokenizer.from_pretrained(model_name)\n    \nmodel = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        torch_dtype=torch.bfloat16,\n        device_map=\"auto\",\n        trust_remote_code=True,\n    )","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:29:50.014402Z","iopub.execute_input":"2023-09-21T15:29:50.014769Z","iopub.status.idle":"2023-09-21T15:32:24.705734Z","shell.execute_reply.started":"2023-09-21T15:29:50.014731Z","shell.execute_reply":"2023-09-21T15:32:24.703413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = \"<<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\n</s> [INST] query [/INST] \"\n\npreds1 = []\nfor _, row in tqdm(df.iterrows(), total=len(df)):\n    inputs = tokenizer(prompt.replace('query', row['instruction']) , return_tensors=\"pt\").to(\"cpu\")\n#f\"cuda:{model.device.index}\"\n    with torch.no_grad():\n        output = model.generate(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_length=1024, \n                                return_dict_in_generate=True, output_scores=True)\n\n    first_token_probs = output.scores[0][0]\n    option_scores = first_token_probs[[319, 350, 315, 360, 382]].float().cpu().numpy() #ABCDE\n    pred = np.array([\"A\", \"B\", \"C\", \"D\", \"E\"])[np.argsort(option_scores)[::-1][:3]]\n  #  pred = ' '.join(pred)\n    preds1.append(pred)\n    \n#sub['prediction'] = preds","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:37:26.194250Z","iopub.execute_input":"2023-09-21T15:37:26.194669Z","iopub.status.idle":"2023-09-21T15:37:39.636580Z","shell.execute_reply.started":"2023-09-21T15:37:26.194638Z","shell.execute_reply":"2023-09-21T15:37:39.635646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#len(preds)#[1][0]","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:53:37.304079Z","iopub.execute_input":"2023-09-21T15:53:37.304784Z","iopub.status.idle":"2023-09-21T15:53:37.336699Z","shell.execute_reply.started":"2023-09-21T15:53:37.304752Z","shell.execute_reply":"2023-09-21T15:53:37.335550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df\ndel model\ndel tokenizer\ngc.collect()\ntorch.cuda.empty_cache() # frees up GPU memory","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:27:56.464077Z","iopub.status.idle":"2023-09-21T15:27:56.465479Z","shell.execute_reply.started":"2023-09-21T15:27:56.464729Z","shell.execute_reply":"2023-09-21T15:27:56.464753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import numpy as np\n# import pandas as pd\n\n# import torch\n\n# import os\n# import re\n\n# we need to install the sentence transformer and use its embedding to read the faiss index\n#cp stands for a copy. This command is used to copy files or groups of files or directories. \n# The -r option tells rm to remove directories recursively, and the -f option tells it to force the removal of files and directories that are read-only or do not exist\n!cp -rf /kaggle/input/sentence-transformers-222/sentence-transformers /kaggle/working/sentence-transformers\n!pip install -U /kaggle/working/sentence-transformers\n!pip install ipywidgets\n#installing faiss package for reading faiss wikipedia index\n!pip install -U /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n# as per wikipedia faiss index https://www.kaggle.com/datasets/jjinho/wikipedia-2023-07-faiss-index\nimport faiss\nfrom faiss import write_index, read_index\n\n# For memory cleaning\nimport ctypes\nimport gc # grabage collector\nlibc = ctypes.CDLL(\"libc.so.6\")\n\n# installing langchain package# We will use langchain recursive splitter\n!pip install langchain --no-index --find-links=file:///kaggle/input/llm-pkg/\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n\n\nfrom tqdm.auto import tqdm","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reading the csv file\n\ndf_test = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\")\n#df_train = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/train.csv\")\n#df_train = pd.read_csv(\"/kaggle/input/additional-train-data-for-llm-science-exam/6000_train_examples.csv\")\n","metadata":{"execution":{"iopub.status.busy":"2023-09-18T12:41:35.886057Z","iopub.execute_input":"2023-09-18T12:41:35.886602Z","iopub.status.idle":"2023-09-18T12:41:35.908683Z","shell.execute_reply.started":"2023-09-18T12:41:35.886550Z","shell.execute_reply":"2023-09-18T12:41:35.906948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## READING WIKIPEDIA FILES TO FIND CONTEXT","metadata":{}},{"cell_type":"code","source":"# PART 1 - Searching Wikipedia Titles","metadata":{"execution":{"iopub.status.busy":"2023-09-18T12:41:38.104458Z","iopub.execute_input":"2023-09-18T12:41:38.104816Z","iopub.status.idle":"2023-09-18T12:41:38.112561Z","shell.execute_reply.started":"2023-09-18T12:41:38.104787Z","shell.execute_reply":"2023-09-18T12:41:38.111523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loadding the wikipedia faiss index. This will be used for searching\nsentence_index = read_index(\"/kaggle/input/wikipedia-2023-07-faiss-index/wikipedia_202307.index\")","metadata":{"execution":{"iopub.status.busy":"2023-09-18T12:41:38.575820Z","iopub.execute_input":"2023-09-18T12:41:38.576162Z","iopub.status.idle":"2023-09-18T12:43:05.441746Z","shell.execute_reply.started":"2023-09-18T12:41:38.576133Z","shell.execute_reply":"2023-09-18T12:43:05.440646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating index of prompts i.e q to search for relavnt wikipedia documents\nfrom sentence_transformers import SentenceTransformer\nSIM_MODEL = '/kaggle/input/sentencetransformers-allminilml6v2/sentence-transformers_all-MiniLM-L6-v2'\nDEVICE = 0\nMAX_LENGTH = 384\nBATCH_SIZE = 16\n\nmodel = SentenceTransformer(SIM_MODEL, device='cuda')\nmodel.max_seq_length = MAX_LENGTH\nmodel = model.half() # The model.half() method in PyTorch is used to convert a model to half-precision. This can be useful for reducing the memory footprint of a model, as half-precision numbers use half the memory as single-precision numbers","metadata":{"execution":{"iopub.status.busy":"2023-09-18T12:43:28.886502Z","iopub.execute_input":"2023-09-18T12:43:28.886933Z","iopub.status.idle":"2023-09-18T12:43:32.150293Z","shell.execute_reply.started":"2023-09-18T12:43:28.886891Z","shell.execute_reply":"2023-09-18T12:43:32.149167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt_embeddings_test = model.encode(df_test['prompt'].values, batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\nprompt_embeddings_test = prompt_embeddings_test.detach().cpu().numpy() # detach to remove gradients.\nsearch_score_test, search_index_test = sentence_index.search(prompt_embeddings_test, 5)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T12:43:35.114553Z","iopub.execute_input":"2023-09-18T12:43:35.114899Z","iopub.status.idle":"2023-09-18T12:43:58.784997Z","shell.execute_reply.started":"2023-09-18T12:43:35.114872Z","shell.execute_reply":"2023-09-18T12:43:58.783818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndel sentence_index # deleting as not required. otherwise it will give memory issue\ndel prompt_embeddings_test\n","metadata":{"execution":{"iopub.status.busy":"2023-09-18T12:44:23.677855Z","iopub.execute_input":"2023-09-18T12:44:23.678275Z","iopub.status.idle":"2023-09-18T12:44:24.268907Z","shell.execute_reply.started":"2023-09-18T12:44:23.678242Z","shell.execute_reply":"2023-09-18T12:44:24.267409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect() # garbage collector..frees up RAM memmory\nlibc.malloc_trim(0)\ntorch.cuda.empty_cache() # frees up GPU memory","metadata":{"execution":{"iopub.status.busy":"2023-09-18T12:44:34.128514Z","iopub.execute_input":"2023-09-18T12:44:34.128967Z","iopub.status.idle":"2023-09-18T12:44:34.543151Z","shell.execute_reply.started":"2023-09-18T12:44:34.128932Z","shell.execute_reply":"2023-09-18T12:44:34.542241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PART 2 - Fetching relavant text of wikipedia documents","metadata":{"execution":{"iopub.status.busy":"2023-09-18T12:44:35.735931Z","iopub.execute_input":"2023-09-18T12:44:35.736292Z","iopub.status.idle":"2023-09-18T12:44:35.740455Z","shell.execute_reply.started":"2023-09-18T12:44:35.736263Z","shell.execute_reply":"2023-09-18T12:44:35.739553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n# getting wikipedia documents \ndef wiki_context(search_score,search_index):\n    df_wiki = pd.read_parquet(\"/kaggle/input/wikipedia-20230701/wiki_2023_index.parquet\",\n                         columns=['id', 'file'])\n    wikipedia_file_data = []\n\n    for i, (scr, idx) in tqdm(enumerate(zip(search_score, search_index)), total=len(search_score)):\n        scr_idx = idx\n        _df = df_wiki.loc[scr_idx].copy()\n        _df['prompt_id'] = i\n        wikipedia_file_data.append(_df)\n    wikipedia_file_data = pd.concat(wikipedia_file_data).reset_index(drop=True)\n    wikipedia_file_data = wikipedia_file_data[['id', 'prompt_id', 'file']].drop_duplicates().sort_values(['file', 'id']).reset_index(drop=True)\n\n    WIKI_PATH = \"/kaggle/input/wikipedia-20230701\"\n    wiki_files = os.listdir(WIKI_PATH)\n\n    wiki_text_data = []\n\n    for file in tqdm(wikipedia_file_data.file.unique(), total=len(wikipedia_file_data.file.unique())):\n        _id = [str(i) for i in wikipedia_file_data[wikipedia_file_data['file']==file]['id'].tolist()]\n        _df = pd.read_parquet(f\"{WIKI_PATH}/{file}\", columns=['id', 'text'])\n\n        _df_temp = _df[_df['id'].isin(_id)].copy()\n        del _df\n        _ = gc.collect()\n        libc.malloc_trim(0)\n        wiki_text_data.append(_df_temp)\n    wiki_text_data = pd.concat(wiki_text_data).drop_duplicates().reset_index(drop=True)\n    del df_wiki\n    _ = gc.collect()\n    libc.malloc_trim(0)\n    context_df = wikipedia_file_data.merge(wiki_text_data,on='id')\n    return context_df","metadata":{"execution":{"iopub.status.busy":"2023-09-18T12:44:36.766688Z","iopub.execute_input":"2023-09-18T12:44:36.767062Z","iopub.status.idle":"2023-09-18T12:44:36.778307Z","shell.execute_reply.started":"2023-09-18T12:44:36.767024Z","shell.execute_reply":"2023-09-18T12:44:36.777282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"context_df_test = wiki_context(search_score_test,search_index_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T12:51:20.983246Z","iopub.execute_input":"2023-09-18T12:51:20.983610Z","iopub.status.idle":"2023-09-18T12:55:48.052626Z","shell.execute_reply.started":"2023-09-18T12:51:20.983578Z","shell.execute_reply":"2023-09-18T12:55:48.051530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\ndef format_dataframe(df, context_df):\n    \n    model = SentenceTransformer(SIM_MODEL, device='cuda')\n    model.max_seq_length = 384\n    model = model.half() # The model.half() method in PyTorch is used to convert a model to half-precision. This can be useful for reducing the memory footprint of a model, as half-precision numbers use half the memory as single-precision numbers\n    \n #   final_prompt = []\n    #for i in range(5):\n    for i in range(len(df)):\n        q = df.iloc[i]['prompt']\n        idx = df.iloc[i]['id']\n        chunk = ''\n        \n        text_rel = context_df[context_df['prompt_id'] == idx].iloc[:]['split']\n        text = []\n        for j in range(len(text_rel)):\n            text.extend(text_rel.iloc[j])\n        if text != []:\n            text_df = pd.DataFrame(text,columns=['text'])\n            vectors = model.encode(text_df['text'])\n            vector_dimension = vectors.shape[1]\n            index = faiss.IndexFlatL2(vector_dimension)\n            faiss.normalize_L2(vectors)\n            index.add(vectors)\n\n\n            search_vector = model.encode(q)\n            _vector = np.array([search_vector])\n            faiss.normalize_L2(_vector)\n\n            k = 1\n            distances, ann = index.search(_vector, k=k)\n            chunk = text[ann[0,0]]\n            chunk = re.sub('[^a-zA-Z0-9 \\n\\.]', '', chunk)\n            \n            chunk2 = text[ann[0,1]]\n            chunk2 = re.sub('[^a-zA-Z0-9 \\n\\.]', '', chunk2)\n\n        df.iloc[i,8] = str(chunk)\n        df.iloc[i,9] = str(chunk2)\n    \n    del index\n    del search_vector\n    del model\n    _ = gc.collect()\n    libc.malloc_trim(0)\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2023-09-18T12:55:59.836833Z","iopub.execute_input":"2023-09-18T12:55:59.837250Z","iopub.status.idle":"2023-09-18T12:55:59.851612Z","shell.execute_reply.started":"2023-09-18T12:55:59.837215Z","shell.execute_reply":"2023-09-18T12:55:59.850370Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TRAINING THE MODEL","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n\nfrom typing import Optional, Union\nimport pandas as pd, numpy as np, torch\nfrom datasets import Dataset\nfrom dataclasses import dataclass\nfrom transformers import AutoTokenizer\nfrom transformers import EarlyStoppingCallback\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\nfrom transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n\n\nVER=10\n# TRAIN WITH SUBSET OF 60K\nNUM_TRAIN_SAMPLES = 2_048#1_024\n# PARAMETER EFFICIENT FINE TUNING\n# PEFT REQUIRES 1XP100 GPU NOT 2XT4\nUSE_PEFT = False\n# NUMBER OF LAYERS TO FREEZE \n# DEBERTA LARGE HAS TOTAL OF 24 LAYERS\nFREEZE_LAYERS = 18#18\n# BOOLEAN TO FREEZE EMBEDDINGS\nFREEZE_EMBEDDINGS = True\n# LENGTH OF CONTEXT PLUS QUESTION ANSWER\nMAX_INPUT = 256\n# HUGGING FACE MODEL\nMODEL = \"/kaggle/input/LLMQAModelV11/Modelv11/Modelv11\"#'microsoft/deberta-v3-large'\nmodel = AutoModelForMultipleChoice.from_pretrained(MODEL)\ntokenizer = AutoTokenizer.from_pretrained(MODEL)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:38:32.278089Z","iopub.execute_input":"2023-09-21T15:38:32.278506Z","iopub.status.idle":"2023-09-21T15:39:00.304694Z","shell.execute_reply.started":"2023-09-21T15:38:32.278477Z","shell.execute_reply":"2023-09-21T15:39:00.302509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"option_to_index = {option: idx for idx, option in enumerate('ABCDE')}\nindex_to_option = {v: k for k,v in option_to_index.items()}\n\ndef preprocess(example):\n    first_sentence = [ \"[CLS] \" + example['context'] ] * 5\n    second_sentences = [\" #### \" + str(example['prompt']) + \" [SEP] \" + str(example[option]) + \" [SEP]\" for option in 'ABCDE']\n    tokenized_example = tokenizer(first_sentence, second_sentences, truncation='only_first', \n                                  max_length=MAX_INPUT,padding='max_length', add_special_tokens=False)\n    tokenized_example['label'] = option_to_index[example['answer']]\n    \n    return tokenized_example\n\n@dataclass\nclass DataCollatorForMultipleChoice:\n    tokenizer: PreTrainedTokenizerBase\n    padding: Union[bool, str, PaddingStrategy] = True\n    max_length: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n    \n    def __call__(self, features):\n        label_name = 'label' if 'label' in features[0].keys() else 'labels'\n        labels = [feature.pop(label_name) for feature in features]\n        batch_size = len(features)\n        num_choices = len(features[0]['input_ids'])\n        flattened_features = [\n            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n        ]\n        flattened_features = sum(flattened_features, [])\n        \n        batch = self.tokenizer.pad(\n            flattened_features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors='pt',\n        )\n        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n        return batch","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:39:28.118169Z","iopub.execute_input":"2023-09-21T15:39:28.119288Z","iopub.status.idle":"2023-09-21T15:39:28.133440Z","shell.execute_reply.started":"2023-09-21T15:39:28.119249Z","shell.execute_reply":"2023-09-21T15:39:28.132253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def map_at_3(predictions, labels):\n    map_sum = 0\n    pred = np.argsort(-1*np.array(predictions),axis=1)[:,:3]\n    for x,y in zip(pred,labels):\n        z = [1/i if y==j else 0 for i,j in zip([1,2,3],x)]\n        map_sum += np.sum(z)\n    return map_sum / len(predictions)\n\ndef compute_metrics(p):\n    predictions = p.predictions.tolist()\n    labels = p.label_ids.tolist()\n    return {\"map@3\": map_at_3(predictions, labels)}\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:39:28.970705Z","iopub.execute_input":"2023-09-21T15:39:28.971067Z","iopub.status.idle":"2023-09-21T15:39:28.978366Z","shell.execute_reply.started":"2023-09-21T15:39:28.971039Z","shell.execute_reply":"2023-09-21T15:39:28.977127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(model=model)\n\n# del model, trainer\n# if USE_PEFT:\n#     model = AutoModelForMultipleChoice.from_pretrained(MODEL)\n#     model = get_peft_model(model, peft_config)\n#     checkpoint = torch.load(f'model_v{VER}/pytorch_model.bin')\n#     model.load_state_dict(checkpoint)\n# else:\n#     model = AutoModelForMultipleChoice.from_pretrained(f'model_v{VER}')\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:39:29.837892Z","iopub.execute_input":"2023-09-21T15:39:29.838274Z","iopub.status.idle":"2023-09-21T15:39:30.334509Z","shell.execute_reply.started":"2023-09-21T15:39:29.838234Z","shell.execute_reply":"2023-09-21T15:39:30.333359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#context_df_test = pd.read_csv('/kaggle/input/context2/context_train.csv')\n#context_df_test = context_df_test1[:5]","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:42:22.950785Z","iopub.execute_input":"2023-09-21T15:42:22.951291Z","iopub.status.idle":"2023-09-21T15:42:22.998666Z","shell.execute_reply.started":"2023-09-21T15:42:22.951242Z","shell.execute_reply":"2023-09-21T15:42:22.997604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## INFERENCING ON TEST DF","metadata":{}},{"cell_type":"code","source":"#df_test = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chunk_size = 2200\nchunk_overlap = 400\n\nr_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=chunk_size,\n    chunk_overlap=chunk_overlap\n)\n\nsplit_text =[]\nfor i in range(len(context_df_test)):\n    split_text.append ( r_splitter.split_text(context_df_test.loc[i,'text']))\ncontext_df_test['split'] = split_text","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:42:25.085916Z","iopub.execute_input":"2023-09-21T15:42:25.086287Z","iopub.status.idle":"2023-09-21T15:42:25.137038Z","shell.execute_reply.started":"2023-09-21T15:42:25.086258Z","shell.execute_reply":"2023-09-21T15:42:25.134774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df_test = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\")\ndf_test['answer'] = 'B' # dummy answer.to make df same as train df\ndf_test['context']= ' ' # making context column to be filled using context_df_test below\ndf_test['context2'] = ' '\ntest_df = format_dataframe(df_test,context_df_test) # updatin df_test with context from context_df_test using faiss","metadata":{"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds = Dataset.from_pandas(test_df) # creating tokenzied test_df to be used in predictions\ntokenized_test_ds = test_ds.map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer','context','context2'])","metadata":{"execution":{"iopub.status.busy":"2023-09-18T13:00:31.319975Z","iopub.execute_input":"2023-09-18T13:00:31.320395Z","iopub.status.idle":"2023-09-18T13:00:32.748458Z","shell.execute_reply.started":"2023-09-18T13:00:31.320352Z","shell.execute_reply":"2023-09-18T13:00:32.747493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntest_predictions = trainer.predict(tokenized_test_ds).predictions\npredictions_as_ids = np.argsort(-test_predictions, 1)\npredictions_as_answer_letters = np.array(list('ABCDE'))[predictions_as_ids]\npredictions_as_string = test_df['prediction'] = [\n    ' '.join(row) for row in predictions_as_answer_letters[:, :3]\n]","metadata":{"execution":{"iopub.status.busy":"2023-09-18T13:01:45.815603Z","iopub.execute_input":"2023-09-18T13:01:45.816021Z","iopub.status.idle":"2023-09-18T13:02:21.910604Z","shell.execute_reply.started":"2023-09-18T13:01:45.815981Z","shell.execute_reply":"2023-09-18T13:02:21.909719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The following function gets the indices of the highest scoring answers for each row\n# and converts them back to our answer format (A, B, C, D, E)\nimport numpy as np\ndef predictions_to_map_output(predictions):\n    sorted_answer_indices = np.argsort(-predictions)\n    top_answer_indices = sorted_answer_indices[:,:3] # Get the first three answers in each row\n    top_answers = np.vectorize(index_to_option.get)(top_answer_indices)\n    return np.apply_along_axis(lambda row: ' '.join(row), 1, top_answers)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-18T13:02:24.947860Z","iopub.execute_input":"2023-09-18T13:02:24.948581Z","iopub.status.idle":"2023-09-18T13:02:24.955519Z","shell.execute_reply.started":"2023-09-18T13:02:24.948541Z","shell.execute_reply":"2023-09-18T13:02:24.954216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making submission df\nsubmission_df = test_df[['id']]\nsubmission_df['prediction'] = predictions_to_map_output(test_predictions)\n\n#submission_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T13:02:26.290452Z","iopub.execute_input":"2023-09-18T13:02:26.290826Z","iopub.status.idle":"2023-09-18T13:02:26.309294Z","shell.execute_reply.started":"2023-09-18T13:02:26.290798Z","shell.execute_reply":"2023-09-18T13:02:26.307918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_test = pd.read_csv(\"/kaggle/input/context2/context_train.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-09-24T05:07:33.614687Z","iopub.execute_input":"2023-09-24T05:07:33.615245Z","iopub.status.idle":"2023-09-24T05:07:33.692365Z","shell.execute_reply.started":"2023-09-24T05:07:33.615205Z","shell.execute_reply":"2023-09-24T05:07:33.691336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_test","metadata":{"execution":{"iopub.status.busy":"2023-09-24T05:17:22.006967Z","iopub.execute_input":"2023-09-24T05:17:22.007383Z","iopub.status.idle":"2023-09-24T05:17:22.013797Z","shell.execute_reply.started":"2023-09-24T05:17:22.007337Z","shell.execute_reply":"2023-09-24T05:17:22.012531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n# from tqdm.auto import tqdm\n# import torch","metadata":{"execution":{"iopub.status.busy":"2023-09-24T05:14:21.271213Z","iopub.execute_input":"2023-09-24T05:14:21.271595Z","iopub.status.idle":"2023-09-24T05:14:21.277617Z","shell.execute_reply.started":"2023-09-24T05:14:21.271566Z","shell.execute_reply":"2023-09-24T05:14:21.276466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import LongformerTokenizer, LongformerForMultipleChoice","metadata":{"execution":{"iopub.status.busy":"2023-09-24T05:02:21.374835Z","iopub.execute_input":"2023-09-24T05:02:21.375264Z","iopub.status.idle":"2023-09-24T05:02:37.194403Z","shell.execute_reply.started":"2023-09-24T05:02:21.375238Z","shell.execute_reply":"2023-09-24T05:02:37.193418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = LongformerTokenizer.from_pretrained(\"/kaggle/input/longformer-race-model/longformer_qa_model\")\nmodel = LongformerForMultipleChoice.from_pretrained(\"/kaggle/input/longformer-race-model/longformer_qa_model\").cuda()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T05:05:09.163748Z","iopub.execute_input":"2023-09-24T05:05:09.164234Z","iopub.status.idle":"2023-09-24T05:05:35.707931Z","shell.execute_reply.started":"2023-09-24T05:05:09.164196Z","shell.execute_reply":"2023-09-24T05:05:35.706790Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_answering_input(\n        tokenizer, \n        question,  \n        options,   \n        context,   \n        max_seq_length=4096,\n    ):\n    c_plus_q   = context + ' ' + tokenizer.bos_token + ' ' + question\n    c_plus_q_4 = [c_plus_q] * len(options)\n    tokenized_examples = tokenizer(\n        c_plus_q_4, options,\n        max_length=max_seq_length,\n        padding=\"longest\",\n        truncation=False,\n        return_tensors=\"pt\",\n    )\n    input_ids = tokenized_examples['input_ids'].unsqueeze(0)\n    attention_mask = tokenized_examples['attention_mask'].unsqueeze(0)\n    example_encoded = {\n        \"input_ids\": input_ids.to(model.device.index),\n        \"attention_mask\": attention_mask.to(model.device.index),\n    }\n    return example_encoded","metadata":{"execution":{"iopub.status.busy":"2023-09-24T05:09:07.651592Z","iopub.execute_input":"2023-09-24T05:09:07.652580Z","iopub.status.idle":"2023-09-24T05:09:07.660535Z","shell.execute_reply.started":"2023-09-24T05:09:07.652529Z","shell.execute_reply":"2023-09-24T05:09:07.659095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds2 = []\nsubmit_ids = []\n\nfor index in tqdm(range(test_df.shape[0])):\n    columns = test_df.iloc[index].values\n    submit_ids.append(columns[0])\n    question = test_df.iloc[index]['prompt']\n    options = [test_df.iloc[index]['A'],test_df.iloc[index]['B'],test_df.iloc[index]['C'],test_df.iloc[index]['D'],test_df.iloc[index]['E']]\n    context1 = test_df.iloc[index]['context']\n    context2 = test_df.iloc[index]['context2']\n    inputs1 = prepare_answering_input(\n        tokenizer=tokenizer, question=question,\n        options=options, context=context1,\n        )\n    inputs2 = prepare_answering_input(\n         tokenizer=tokenizer, question=question,\n         options=options, context=context2,\n         )\n    \n    with torch.no_grad():\n        outputs1 = model(**inputs1)    \n        losses1 = -outputs1.logits[0].detach().cpu().numpy()\n        probability1 = torch.softmax(torch.tensor(-losses1), dim=-1)\n        \n    with torch.no_grad():\n        outputs2 = model(**inputs2)\n        losses2 = -outputs2.logits[0].detach().cpu().numpy()\n        probability2 = torch.softmax(torch.tensor(-losses2), dim=-1)\n    \n    \n     probability_ = (probability1 + probability2)/2\n\n#     #if probability_.max() > 0.4:\n#         predict = np.array(list(\"ABCDE\"))[np.argsort(probability_)][-3:].tolist()[::-1]\n#     else:\n#         predict = backup_model_predictions.iloc[index].prediction.replace(\" \",\"\")\n#     predictions.append(predict)\n    predict = np.array(list(\"ABCDE\"))[np.argsort(probability_)][-3:].tolist()[::-1]\n    preds2.append(predict)\n#predictions = [\" \".join(i) for i in predictions]\n","metadata":{"execution":{"iopub.status.busy":"2023-09-24T05:16:03.188043Z","iopub.execute_input":"2023-09-24T05:16:03.188420Z","iopub.status.idle":"2023-09-24T05:16:07.461862Z","shell.execute_reply.started":"2023-09-24T05:16:03.188385Z","shell.execute_reply":"2023-09-24T05:16:07.460585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds2[1]","metadata":{"execution":{"iopub.status.busy":"2023-09-24T05:16:24.319493Z","iopub.execute_input":"2023-09-24T05:16:24.319852Z","iopub.status.idle":"2023-09-24T05:16:24.329298Z","shell.execute_reply.started":"2023-09-24T05:16:24.319823Z","shell.execute_reply":"2023-09-24T05:16:24.328025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter \ndef final_pred(sub_df, llama_preds1,llama_preds2):\n    for i in range(len(sub_df)):\n        final_ans = []\n        \n        x1 = sub_df.iloc[i,1].split()\n        final_ans.extend(x1)\n        \n        final_ans.extend(llama_preds1[i])\n            \n        final_ans.extend(llama_preds2[i])\n        \n     #   print(final_ans)\n        pred_final = Counter(final_ans)\n        \n        pred_final = sorted(pred_final.items(), key=lambda x : x[1],reverse=True)\n        \n        pred_final_str = pred_final[0][0] + ' ' + pred_final[1][0] + ' ' + pred_final[2][0]\n        \n      #  print(pred_final)\n        sub_df.iloc[i,1] = pred_final_str\n    return sub_df\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_df = final_pred(submission_df,preds1,preds2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def check(sub_df, llama_preds):\n#     for i in range(len(sub_df)):\n#         x = sub_df.iloc[i,1].split()\n#         if llama_preds[i][0] not in x:\n#             x[2] = llama_preds[i][0]\n#             sub_df.iloc[i,1] = \" \".join(x)\n#     return sub_df\n\n\n    ","metadata":{"execution":{"iopub.status.busy":"2023-09-21T16:22:57.855231Z","iopub.execute_input":"2023-09-21T16:22:57.855709Z","iopub.status.idle":"2023-09-21T16:22:57.863311Z","shell.execute_reply.started":"2023-09-21T16:22:57.855673Z","shell.execute_reply":"2023-09-21T16:22:57.862082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n\n# preds = []\n# x = np.array([\"A\",\"B\",\"C\",\"D\"])\n# preds.append(x)\n\n# preds.append(x)\n# preds.append(x)\n\n# preds\n# x ={'id':[1,2,3],'pred':['A B C','D B C','A B C']}\n# submission_df = pd.DataFrame(data=x)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T16:22:59.584496Z","iopub.execute_input":"2023-09-21T16:22:59.584928Z","iopub.status.idle":"2023-09-21T16:22:59.592203Z","shell.execute_reply.started":"2023-09-21T16:22:59.584896Z","shell.execute_reply":"2023-09-21T16:22:59.590514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# final_df = check(submission_df,preds)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T16:23:12.652824Z","iopub.execute_input":"2023-09-21T16:23:12.653287Z","iopub.status.idle":"2023-09-21T16:23:12.661026Z","shell.execute_reply.started":"2023-09-21T16:23:12.653254Z","shell.execute_reply":"2023-09-21T16:23:12.659299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Writign submission file to output\nif os.path.exists('submission.csv'):\n    os.remove('submission.csv')\nfinal_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T13:02:31.260711Z","iopub.execute_input":"2023-09-18T13:02:31.261083Z","iopub.status.idle":"2023-09-18T13:02:31.271363Z","shell.execute_reply.started":"2023-09-18T13:02:31.261049Z","shell.execute_reply":"2023-09-18T13:02:31.270209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
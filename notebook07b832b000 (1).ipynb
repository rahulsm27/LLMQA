{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nfrom string import Template # For generating prompt template\n\nimport os\nimport gc # grabage collector\n# we need to install the sentence transformer and use its embedding to read the faiss index\n#cp stands for a copy. This command is used to copy files or groups of files or directories. \n# The -r option tells rm to remove directories recursively, and the -f option tells it to force the removal of files and directories that are read-only or do not exist\n\n!cp -rf /kaggle/input/sentence-transformers-222/sentence-transformers /kaggle/working/sentence-transformers\n!pip install -U /kaggle/working/sentence-transformers\n\n#installing faiss package for reading faiss wikipedia index\n!pip install -U /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n# as per wikipedia faiss index https://www.kaggle.com/datasets/jjinho/wikipedia-2023-07-faiss-index\nimport faiss\nfrom faiss import write_index, read_index\n\n\nimport ctypes\nlibc = ctypes.CDLL(\"libc.so.6\")\n\n# installing langchain package# We will use langchain recursive splitter\n!pip install langchain --no-index --find-links=file:///kaggle/input/llm-pkg/\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n\n\nfrom tqdm.auto import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-09-07T11:37:48.831659Z","iopub.execute_input":"2023-09-07T11:37:48.832584Z","iopub.status.idle":"2023-09-07T11:39:33.069830Z","shell.execute_reply.started":"2023-09-07T11:37:48.832533Z","shell.execute_reply":"2023-09-07T11:39:33.068640Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reading the csv file\n#df_train = pd.read_csv(\"./train.csv\")\ndf_final = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\")\ndf_final.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-09-07T11:39:33.073215Z","iopub.execute_input":"2023-09-07T11:39:33.074357Z","iopub.status.idle":"2023-09-07T11:39:33.121196Z","shell.execute_reply.started":"2023-09-07T11:39:33.074308Z","shell.execute_reply":"2023-09-07T11:39:33.120152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"READING WIKIPEDIA FILES TO FIND CONTEXT****","metadata":{}},{"cell_type":"code","source":"# PART 1 - Searching Wikipedia Titles","metadata":{"execution":{"iopub.status.busy":"2023-09-07T11:39:33.122736Z","iopub.execute_input":"2023-09-07T11:39:33.123106Z","iopub.status.idle":"2023-09-07T11:39:33.129527Z","shell.execute_reply.started":"2023-09-07T11:39:33.123053Z","shell.execute_reply":"2023-09-07T11:39:33.128033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loadding the wikipedia faiss index. This will be used for searching\nsentence_index = read_index(\"/kaggle/input/wikipedia-2023-07-faiss-index/wikipedia_202307.index\")","metadata":{"execution":{"iopub.status.busy":"2023-09-07T11:39:33.133117Z","iopub.execute_input":"2023-09-07T11:39:33.133843Z","iopub.status.idle":"2023-09-07T11:41:04.223984Z","shell.execute_reply.started":"2023-09-07T11:39:33.133805Z","shell.execute_reply":"2023-09-07T11:41:04.222511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating index of prompts i.e q to serach for relavnt wikipedia documents\nfrom sentence_transformers import SentenceTransformer\nSIM_MODEL = '/kaggle/input/sentencetransformers-allminilml6v2/sentence-transformers_all-MiniLM-L6-v2'\nDEVICE = 0\nMAX_LENGTH = 384\nBATCH_SIZE = 32\n\nmodel = SentenceTransformer(SIM_MODEL, device='cuda')\nmodel.max_seq_length = MAX_LENGTH\nmodel = model.half() # The model.half() method in PyTorch is used to convert a model to half-precision. This can be useful for reducing the memory footprint of a model, as half-precision numbers use half the memory as single-precision numbers","metadata":{"execution":{"iopub.status.busy":"2023-09-07T11:41:04.225931Z","iopub.execute_input":"2023-09-07T11:41:04.226792Z","iopub.status.idle":"2023-09-07T11:41:07.686274Z","shell.execute_reply.started":"2023-09-07T11:41:04.226751Z","shell.execute_reply":"2023-09-07T11:41:07.685149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt_embeddings = model.encode(df_final['prompt'].values, batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\nprompt_embeddings = prompt_embeddings.detach().cpu().numpy() # detach to remove gradients.\nsearch_score, search_index = sentence_index.search(prompt_embeddings, 3)","metadata":{"execution":{"iopub.status.busy":"2023-09-07T11:41:07.687622Z","iopub.execute_input":"2023-09-07T11:41:07.688013Z","iopub.status.idle":"2023-09-07T11:41:30.400534Z","shell.execute_reply.started":"2023-09-07T11:41:07.687975Z","shell.execute_reply":"2023-09-07T11:41:30.399434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndel sentence_index # deleting as not required. otherwise it will give memory issue\ndel prompt_embeddings\n_ = gc.collect() # garbage collector..frees up memmory","metadata":{"execution":{"iopub.status.busy":"2023-09-07T11:41:30.401978Z","iopub.execute_input":"2023-09-07T11:41:30.403009Z","iopub.status.idle":"2023-09-07T11:41:31.338958Z","shell.execute_reply.started":"2023-09-07T11:41:30.402967Z","shell.execute_reply":"2023-09-07T11:41:31.337728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PART 2 - Fetching relavant text of wikipedia documents","metadata":{"execution":{"iopub.status.busy":"2023-09-07T11:41:31.340556Z","iopub.execute_input":"2023-09-07T11:41:31.340917Z","iopub.status.idle":"2023-09-07T11:41:31.352166Z","shell.execute_reply.started":"2023-09-07T11:41:31.340881Z","shell.execute_reply":"2023-09-07T11:41:31.351028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# getting wikipedia documents \ndf_wiki = pd.read_parquet(\"/kaggle/input/wikipedia-20230701/wiki_2023_index.parquet\",\n                     columns=['id', 'file'])","metadata":{"execution":{"iopub.status.busy":"2023-09-07T11:41:31.353730Z","iopub.execute_input":"2023-09-07T11:41:31.354028Z","iopub.status.idle":"2023-09-07T11:41:36.576173Z","shell.execute_reply.started":"2023-09-07T11:41:31.354002Z","shell.execute_reply":"2023-09-07T11:41:36.575145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wikipedia_file_data = []\n\nfor i, (scr, idx) in tqdm(enumerate(zip(search_score, search_index)), total=len(search_score)):\n    scr_idx = idx\n    _df = df_wiki.loc[scr_idx].copy()\n    _df['prompt_id'] = i\n    wikipedia_file_data.append(_df)","metadata":{"execution":{"iopub.status.busy":"2023-09-07T11:42:25.075753Z","iopub.execute_input":"2023-09-07T11:42:25.076163Z","iopub.status.idle":"2023-09-07T11:42:25.264508Z","shell.execute_reply.started":"2023-09-07T11:42:25.076127Z","shell.execute_reply":"2023-09-07T11:42:25.263443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wikipedia_file_data = pd.concat(wikipedia_file_data).reset_index(drop=True)\nwikipedia_file_data = wikipedia_file_data[['id', 'prompt_id', 'file']].drop_duplicates().sort_values(['file', 'id']).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-07T11:42:26.747588Z","iopub.execute_input":"2023-09-07T11:42:26.747953Z","iopub.status.idle":"2023-09-07T11:42:26.782233Z","shell.execute_reply.started":"2023-09-07T11:42:26.747919Z","shell.execute_reply":"2023-09-07T11:42:26.781254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wikipedia_file_data.head(5)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-07T11:44:18.333255Z","iopub.execute_input":"2023-09-07T11:44:18.334517Z","iopub.status.idle":"2023-09-07T11:44:18.347960Z","shell.execute_reply.started":"2023-09-07T11:44:18.334475Z","shell.execute_reply":"2023-09-07T11:44:18.346758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df_wiki\n_ = gc.collect()\nlibc.malloc_trim(0)","metadata":{"execution":{"iopub.status.busy":"2023-09-07T11:43:16.135793Z","iopub.execute_input":"2023-09-07T11:43:16.136210Z","iopub.status.idle":"2023-09-07T11:43:16.710099Z","shell.execute_reply.started":"2023-09-07T11:43:16.136175Z","shell.execute_reply":"2023-09-07T11:43:16.709039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nWIKI_PATH = \"/kaggle/input/wikipedia-20230701\"\nwiki_files = os.listdir(WIKI_PATH)\n\nwiki_text_data = []\n\nfor file in tqdm(wikipedia_file_data.file.unique(), total=len(wikipedia_file_data.file.unique())):\n    _id = [str(i) for i in wikipedia_file_data[wikipedia_file_data['file']==file]['id'].tolist()]\n    _df = pd.read_parquet(f\"{WIKI_PATH}/{file}\", columns=['id', 'text'])\n\n    _df_temp = _df[_df['id'].isin(_id)].copy()\n    del _df\n    _ = gc.collect()\n    libc.malloc_trim(0)\n    wiki_text_data.append(_df_temp)\nwiki_text_data = pd.concat(wiki_text_data).drop_duplicates().reset_index(drop=True)\n_ = gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-09-07T11:44:58.393470Z","iopub.execute_input":"2023-09-07T11:44:58.393939Z","iopub.status.idle":"2023-09-07T11:49:25.565330Z","shell.execute_reply.started":"2023-09-07T11:44:58.393903Z","shell.execute_reply":"2023-09-07T11:49:25.564246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"context_df = wikipedia_file_data.merge(wiki_text_data,on='id')\nprint(len(context_df))","metadata":{"execution":{"iopub.status.busy":"2023-09-07T11:52:07.604416Z","iopub.execute_input":"2023-09-07T11:52:07.604875Z","iopub.status.idle":"2023-09-07T11:52:07.623854Z","shell.execute_reply.started":"2023-09-07T11:52:07.604838Z","shell.execute_reply":"2023-09-07T11:52:07.622749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"context_df.head()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-07T11:52:08.887891Z","iopub.execute_input":"2023-09-07T11:52:08.889404Z","iopub.status.idle":"2023-09-07T11:52:08.903686Z","shell.execute_reply.started":"2023-09-07T11:52:08.889354Z","shell.execute_reply":"2023-09-07T11:52:08.902580Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Spliting the wiki text in the context df in chunk size\n\nchunk_size = 1000\nchunk_overlap = 100\n\nr_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=chunk_size,\n    chunk_overlap=chunk_overlap\n)\n\nsplit_text =[]\nfor i in range(len(context_df)):\n    split_text.append ( r_splitter.split_text(context_df.loc[i,'text']))\ncontext_df['split'] = split_text\n","metadata":{"execution":{"iopub.status.busy":"2023-09-07T11:52:20.245300Z","iopub.execute_input":"2023-09-07T11:52:20.245686Z","iopub.status.idle":"2023-09-07T11:52:22.918565Z","shell.execute_reply.started":"2023-09-07T11:52:20.245645Z","shell.execute_reply":"2023-09-07T11:52:22.917543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"context_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-07T11:52:30.546069Z","iopub.execute_input":"2023-09-07T11:52:30.547007Z","iopub.status.idle":"2023-09-07T11:52:30.579133Z","shell.execute_reply.started":"2023-09-07T11:52:30.546961Z","shell.execute_reply":"2023-09-07T11:52:30.578152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"PREPARING THE PROMPT","metadata":{}},{"cell_type":"code","source":"text  = \"\"\"\nGive below is a question labelled as 'Q' : and 5 possible answers to the question labelled as 'A':,'B':,'C':,'D':,'E':. \nYour task is to predict the top 3 most likely answer to the question.\nYour output should cosist be 3 letters from A,B,C,D,E. The first letter should indicate the most liley answer folloed by 2nd most likely answer followed by 3rd most likely answer.\nUse context labeled as 'T' for any relavant information\n\n'Q' : $q\n\n'A' : $a\n'B' : $b\n'C' : $c\n'D' : $d\n'E' : $e\n\n'T' : $t\n\n\n\"\"\"\n\ntemplate = Template(text)","metadata":{"execution":{"iopub.status.busy":"2023-09-07T11:52:36.719457Z","iopub.execute_input":"2023-09-07T11:52:36.719853Z","iopub.status.idle":"2023-09-07T11:52:36.725622Z","shell.execute_reply.started":"2023-09-07T11:52:36.719818Z","shell.execute_reply":"2023-09-07T11:52:36.724417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_dataframe(df, context_df):\n    \n    final_prompt = []\n    for i in range(len(df)):\n        q = df.loc[i,'prompt']\n        a = df.loc[i,'A']\n        b = df.loc[i,'B']\n        c = df.loc[i,'C']\n        d = df.loc[i,'D']\n        e = df.loc[i,'E']\n        \n        text = context_df[context_df['prompt_id'] == i].iloc[0]['split']\n\n        text_df = pd.DataFrame(text,columns=['text'])\n        vectors = model.encode(text_df['text'])\n        vector_dimension = vectors.shape[1]\n        index = faiss.IndexFlatL2(vector_dimension)\n        faiss.normalize_L2(vectors)\n        index.add(vectors)\n\n    \n        search_vector = model.encode(q)\n        _vector = np.array([search_vector])\n        faiss.normalize_L2(_vector)\n\n        k = 1\n        distances, ann = index.search(_vector, k=k)\n        chunk = text[ann[0,0]]\n\n        final_prompt.append(template.substitute(q=q,a = a,b=b,c=c,d=d,e=e,t=chunk))\n     #   break\n    \n    return final_prompt\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-07T11:55:59.660493Z","iopub.execute_input":"2023-09-07T11:55:59.660878Z","iopub.status.idle":"2023-09-07T11:55:59.671126Z","shell.execute_reply.started":"2023-09-07T11:55:59.660845Z","shell.execute_reply":"2023-09-07T11:55:59.670137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_prompt = format_dataframe(df_final,context_df)","metadata":{"execution":{"iopub.status.busy":"2023-09-07T11:56:00.572437Z","iopub.execute_input":"2023-09-07T11:56:00.572807Z","iopub.status.idle":"2023-09-07T11:56:20.726219Z","shell.execute_reply.started":"2023-09-07T11:56:00.572776Z","shell.execute_reply":"2023-09-07T11:56:20.725079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"PROMPTING THE MODEL TO GET RESPONSE","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\nllm = '/kaggle/input/flan-t5/pytorch/small/2'\n\nmodel_llm = T5ForConditionalGeneration.from_pretrained(llm,local_files_only = True).to(device)\ntokenizer = T5Tokenizer.from_pretrained(llm)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-07T11:56:35.348186Z","iopub.execute_input":"2023-09-07T11:56:35.348592Z","iopub.status.idle":"2023-09-07T11:56:40.486008Z","shell.execute_reply.started":"2023-09-07T11:56:35.348559Z","shell.execute_reply":"2023-09-07T11:56:40.484947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking for one response\n\ninputs = tokenizer(model_prompt[0], return_tensors=\"pt\").to(device)\noutputs = model_llm.generate(**inputs)\nanswer = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\nprint(answer)","metadata":{"execution":{"iopub.status.busy":"2023-09-07T11:56:57.856132Z","iopub.execute_input":"2023-09-07T11:56:57.857069Z","iopub.status.idle":"2023-09-07T11:56:59.159722Z","shell.execute_reply.started":"2023-09-07T11:56:57.857033Z","shell.execute_reply":"2023-09-07T11:56:59.158742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**SUBMISSION **","metadata":{}},{"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/kaggle-llm-science-exam/sample_submission.csv', index_col='id')\n\ni = 0\nfor text in model_prompt:\n    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n    outputs = model_llm.generate(**inputs)\n    answer = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n    submission.loc[i,'prediction'] = answer[0]\n    i = i+1\n\nsubmission.to_csv('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2023-09-07T11:57:15.305388Z","iopub.execute_input":"2023-09-07T11:57:15.305770Z","iopub.status.idle":"2023-09-07T11:57:25.324621Z","shell.execute_reply.started":"2023-09-07T11:57:15.305738Z","shell.execute_reply":"2023-09-07T11:57:25.323598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
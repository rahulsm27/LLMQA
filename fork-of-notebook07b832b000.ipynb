{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nfrom string import Template # For generating prompt template\n\nimport os\nimport gc # grabage collector\n# we need to install the sentence transformer and use its embedding to read the faiss index\n#cp stands for a copy. This command is used to copy files or groups of files or directories. \n# The -r option tells rm to remove directories recursively, and the -f option tells it to force the removal of files and directories that are read-only or do not exist\n\n!cp -rf /kaggle/input/sentence-transformers-222/sentence-transformers /kaggle/working/sentence-transformers\n!pip install -U /kaggle/working/sentence-transformers\n\n#installing faiss package for reading faiss wikipedia index\n!pip install -U /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n# as per wikipedia faiss index https://www.kaggle.com/datasets/jjinho/wikipedia-2023-07-faiss-index\nimport faiss\nfrom faiss import write_index, read_index\n\n\nimport ctypes\nlibc = ctypes.CDLL(\"libc.so.6\")\n\n# installing langchain package# We will use langchain recursive splitter\n!pip install langchain --no-index --find-links=file:///kaggle/input/llm-pkg/\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n\n\nfrom tqdm.auto import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-09-16T14:19:19.770905Z","iopub.execute_input":"2023-09-16T14:19:19.771488Z","iopub.status.idle":"2023-09-16T14:21:18.465331Z","shell.execute_reply.started":"2023-09-16T14:19:19.771455Z","shell.execute_reply":"2023-09-16T14:21:18.464229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reading the csv file\n#df_train = pd.read_csv(\"./train.csv\")\ndf_train = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\")\n#df_train.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T14:21:18.467453Z","iopub.execute_input":"2023-09-16T14:21:18.468149Z","iopub.status.idle":"2023-09-16T14:21:18.507652Z","shell.execute_reply.started":"2023-09-16T14:21:18.468110Z","shell.execute_reply":"2023-09-16T14:21:18.506779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## READING WIKIPEDIA FILES TO FIND CONTEXT","metadata":{}},{"cell_type":"code","source":"# PART 1 - Searching Wikipedia Titles","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loadding the wikipedia faiss index. This will be used for searching\nsentence_index = read_index(\"/kaggle/input/wikipedia-2023-07-faiss-index/wikipedia_202307.index\")","metadata":{"execution":{"iopub.status.busy":"2023-09-16T14:21:18.509274Z","iopub.execute_input":"2023-09-16T14:21:18.509629Z","iopub.status.idle":"2023-09-16T14:23:04.393043Z","shell.execute_reply.started":"2023-09-16T14:21:18.509595Z","shell.execute_reply":"2023-09-16T14:23:04.391926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating index of prompts i.e q to search for relavnt wikipedia documents\nfrom sentence_transformers import SentenceTransformer\nSIM_MODEL = '/kaggle/input/sentencetransformers-allminilml6v2/sentence-transformers_all-MiniLM-L6-v2'\nDEVICE = 0\nMAX_LENGTH = 384\nBATCH_SIZE = 32\n\nmodel = SentenceTransformer(SIM_MODEL, device='cuda')\nmodel.max_seq_length = MAX_LENGTH\nmodel = model.half() # The model.half() method in PyTorch is used to convert a model to half-precision. This can be useful for reducing the memory footprint of a model, as half-precision numbers use half the memory as single-precision numbers","metadata":{"execution":{"iopub.status.busy":"2023-09-16T14:23:04.395936Z","iopub.execute_input":"2023-09-16T14:23:04.396317Z","iopub.status.idle":"2023-09-16T14:23:09.403689Z","shell.execute_reply.started":"2023-09-16T14:23:04.396282Z","shell.execute_reply":"2023-09-16T14:23:09.402682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt_embeddings_train = model.encode(df_train['prompt'].values, batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\nprompt_embeddings_train = prompt_embeddings_train.detach().cpu().numpy() # detach to remove gradients.\nsearch_score_train, search_index_train = sentence_index.search(prompt_embeddings_train, 5)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T14:23:09.405195Z","iopub.execute_input":"2023-09-16T14:23:09.405576Z","iopub.status.idle":"2023-09-16T14:23:36.824265Z","shell.execute_reply.started":"2023-09-16T14:23:09.405545Z","shell.execute_reply":"2023-09-16T14:23:36.823237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del prompt_embeddings_train\n_ = gc.collect() # garbage collector..frees up memmory\nlibc.malloc_trim(0)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T14:23:36.825705Z","iopub.execute_input":"2023-09-16T14:23:36.826071Z","iopub.status.idle":"2023-09-16T14:23:37.145388Z","shell.execute_reply.started":"2023-09-16T14:23:36.826037Z","shell.execute_reply":"2023-09-16T14:23:37.144310Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt_embeddings_test = model.encode(df_test['prompt'].values, batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\nprompt_embeddings_test = prompt_embeddings_test.detach().cpu().numpy() # detach to remove gradients.\nsearch_score_test, search_index_test = sentence_index.search(prompt_embeddings_test, 5)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T14:23:37.147033Z","iopub.execute_input":"2023-09-16T14:23:37.148053Z","iopub.status.idle":"2023-09-16T14:23:50.963835Z","shell.execute_reply.started":"2023-09-16T14:23:37.148019Z","shell.execute_reply":"2023-09-16T14:23:50.962832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndel sentence_index # deleting as not required. otherwise it will give memory issue\n\ndel prompt_embeddings_test\n_ = gc.collect() # garbage collector..frees up memmory\nlibc.malloc_trim(0)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T14:23:50.965250Z","iopub.execute_input":"2023-09-16T14:23:50.965957Z","iopub.status.idle":"2023-09-16T14:23:51.839805Z","shell.execute_reply.started":"2023-09-16T14:23:50.965921Z","shell.execute_reply":"2023-09-16T14:23:51.838763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PART 2 - Fetching relavant text of wikipedia documents","metadata":{"execution":{"iopub.status.busy":"2023-09-16T14:23:51.841242Z","iopub.execute_input":"2023-09-16T14:23:51.841825Z","iopub.status.idle":"2023-09-16T14:23:51.846525Z","shell.execute_reply.started":"2023-09-16T14:23:51.841788Z","shell.execute_reply":"2023-09-16T14:23:51.845473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n# getting wikipedia documents \ndef wiki_context(search_score,search_index):\n    df_wiki = pd.read_parquet(\"/kaggle/input/wikipedia-20230701/wiki_2023_index.parquet\",\n                         columns=['id', 'file'])\n    wikipedia_file_data = []\n\n    for i, (scr, idx) in tqdm(enumerate(zip(search_score, search_index)), total=len(search_score)):\n        scr_idx = idx\n        _df = df_wiki.loc[scr_idx].copy()\n        _df['prompt_id'] = i\n        wikipedia_file_data.append(_df)\n    wikipedia_file_data = pd.concat(wikipedia_file_data).reset_index(drop=True)\n    wikipedia_file_data = wikipedia_file_data[['id', 'prompt_id', 'file']].drop_duplicates().sort_values(['file', 'id']).reset_index(drop=True)\n\n    WIKI_PATH = \"/kaggle/input/wikipedia-20230701\"\n    wiki_files = os.listdir(WIKI_PATH)\n\n    wiki_text_data = []\n\n    for file in tqdm(wikipedia_file_data.file.unique(), total=len(wikipedia_file_data.file.unique())):\n        _id = [str(i) for i in wikipedia_file_data[wikipedia_file_data['file']==file]['id'].tolist()]\n        _df = pd.read_parquet(f\"{WIKI_PATH}/{file}\", columns=['id', 'text'])\n\n        _df_temp = _df[_df['id'].isin(_id)].copy()\n        del _df\n        _ = gc.collect()\n        libc.malloc_trim(0)\n        wiki_text_data.append(_df_temp)\n    wiki_text_data = pd.concat(wiki_text_data).drop_duplicates().reset_index(drop=True)\n    del df_wiki\n    _ = gc.collect()\n    libc.malloc_trim(0)\n    context_df = wikipedia_file_data.merge(wiki_text_data,on='id')\n    return context_df","metadata":{"execution":{"iopub.status.busy":"2023-09-16T14:23:51.851022Z","iopub.execute_input":"2023-09-16T14:23:51.851402Z","iopub.status.idle":"2023-09-16T14:23:51.864253Z","shell.execute_reply.started":"2023-09-16T14:23:51.851358Z","shell.execute_reply":"2023-09-16T14:23:51.863224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# del df_wiki\n# _ = gc.collect()\n# libc.malloc_trim(0)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T14:23:51.865756Z","iopub.execute_input":"2023-09-16T14:23:51.866128Z","iopub.status.idle":"2023-09-16T14:23:51.880718Z","shell.execute_reply.started":"2023-09-16T14:23:51.866092Z","shell.execute_reply":"2023-09-16T14:23:51.879869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"context_df_train = wiki_context(search_score_train,search_index_train)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-16T14:23:51.882063Z","iopub.execute_input":"2023-09-16T14:23:51.882475Z","iopub.status.idle":"2023-09-16T14:28:35.346066Z","shell.execute_reply.started":"2023-09-16T14:23:51.882443Z","shell.execute_reply":"2023-09-16T14:28:35.344916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"context_df_test = wiki_context(search_score_test,search_index_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T14:28:35.347779Z","iopub.execute_input":"2023-09-16T14:28:35.348196Z","iopub.status.idle":"2023-09-16T14:33:24.891683Z","shell.execute_reply.started":"2023-09-16T14:28:35.348155Z","shell.execute_reply":"2023-09-16T14:33:24.890648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Spliting the wiki text in the context df in chunk size\n\nchunk_size = 500\nchunk_overlap = 100\n\nr_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=chunk_size,\n    chunk_overlap=chunk_overlap\n)\n\nsplit_text =[]\nfor i in range(len(context_df_train)):\n    split_text.append ( r_splitter.split_text(context_df_train.loc[i,'text']))\ncontext_df_train['split'] = split_text\n","metadata":{"execution":{"iopub.status.busy":"2023-09-16T14:33:24.893070Z","iopub.execute_input":"2023-09-16T14:33:24.893473Z","iopub.status.idle":"2023-09-16T14:33:28.804822Z","shell.execute_reply.started":"2023-09-16T14:33:24.893440Z","shell.execute_reply":"2023-09-16T14:33:28.803684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_dataframe(df, context_df):\n    \n    model = SentenceTransformer(SIM_MODEL, device='cuda')\n    model.max_seq_length = MAX_LENGTH\n    model = model.half() # The model.half() method in PyTorch is used to convert a model to half-precision. This can be useful for reducing the memory footprint of a model, as half-precision numbers use half the memory as single-precision numbers\n    \n #   final_prompt = []\n    #for i in range(5):\n    for i in range(len(df)):\n        q = df.iloc[i]['prompt']\n        idx = df.iloc[i]['id']\n        chunk = ''\n        \n        text_rel = context_df[context_df['prompt_id'] == idx].iloc[:]['split']\n        text = []\n        for j in range(len(text_rel)):\n            text.extend(text_rel.iloc[j])\n        if text != []:\n            text_df = pd.DataFrame(text,columns=['text'])\n            vectors = model.encode(text_df['text'])\n            vector_dimension = vectors.shape[1]\n            index = faiss.IndexFlatL2(vector_dimension)\n            faiss.normalize_L2(vectors)\n            index.add(vectors)\n\n\n            search_vector = model.encode(q)\n            _vector = np.array([search_vector])\n            faiss.normalize_L2(_vector)\n\n            k = 1\n            distances, ann = index.search(_vector, k=k)\n            chunk = text[ann[0,0]]\n\n        df.iloc[i,1] = str(chunk) + ' ### ' + q\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-09-16T14:33:28.806343Z","iopub.execute_input":"2023-09-16T14:33:28.806706Z","iopub.status.idle":"2023-09-16T14:33:28.819653Z","shell.execute_reply.started":"2023-09-16T14:33:28.806668Z","shell.execute_reply":"2023-09-16T14:33:28.818245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_train_df = format_dataframe(df_train,context_df_train)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T14:33:28.820986Z","iopub.execute_input":"2023-09-16T14:33:28.821854Z","iopub.status.idle":"2023-09-16T14:34:17.819827Z","shell.execute_reply.started":"2023-09-16T14:33:28.821811Z","shell.execute_reply":"2023-09-16T14:34:17.818232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TRAINING THE MODEL","metadata":{}},{"cell_type":"code","source":"#model_train[[\"prompt\", \"context\", \"A\", \"B\", \"C\", \"D\", \"E\",\"answer\"]].to_csv(\"./train_context.csv\", index=False)\n#model_train_df = pd.read_csv(\"train_context.csv\")\n#model_train_df.index = list(range(len(model_train_df)))\n#model_train_df['id'] = list(range(len(model_train_df)))\n#model_train_df['context'] = model_train_df['context'].apply(lambda x: str(x))\n#model_train_df[\"prompt\"] = model_train_df[\"context\"] + \" #### \" +  model_train_df[\"prompt\"]\n#model_train_df['answer'] = 'B'","metadata":{"execution":{"iopub.status.busy":"2023-09-16T14:34:17.821686Z","iopub.execute_input":"2023-09-16T14:34:17.822153Z","iopub.status.idle":"2023-09-16T14:34:17.827916Z","shell.execute_reply.started":"2023-09-16T14:34:17.822110Z","shell.execute_reply":"2023-09-16T14:34:17.826914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer,AutoModelForMultipleChoice,Trainer,TrainingArguments\n# For convenience we'll turn our pandas Dataframe into a Dataset\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer\n\n# The path of the model checkpoint we want to use\nmodel_dir = '/kaggle/input/huggingface-bert/bert-base-cased'\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\n#tokenizer(truncation=True,max_length=512,text_target='None')\nmodel = AutoModelForMultipleChoice.from_pretrained(model_dir).cuda()\nmodel.eval()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-16T14:35:34.121779Z","iopub.execute_input":"2023-09-16T14:35:34.122215Z","iopub.status.idle":"2023-09-16T14:35:35.704286Z","shell.execute_reply.started":"2023-09-16T14:35:34.122184Z","shell.execute_reply":"2023-09-16T14:35:35.703284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = Dataset.from_pandas(model_train_df)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T14:34:24.157012Z","iopub.execute_input":"2023-09-16T14:34:24.157684Z","iopub.status.idle":"2023-09-16T14:34:24.175121Z","shell.execute_reply.started":"2023-09-16T14:34:24.157644Z","shell.execute_reply":"2023-09-16T14:34:24.174007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We'll create a dictionary to convert option names (A, B, C, D, E) into indices and back again\noptions = 'ABCDE'\nindices = list(range(5))\n\noption_to_index = {option: index for option, index in zip(options, indices)}\nindex_to_option = {index: option for option, index in zip(options, indices)}\n\ndef preprocess(example):\n    # The AutoModelForMultipleChoice class expects a set of question/answer pairs\n    # so we'll copy our question 5 times before tokenizing\n    first_sentence = [example['prompt']] * 5\n    second_sentence = []\n    for option in options:\n        second_sentence.append(example[option])\n    # Our tokenizer will turn our text into token IDs BERT can understand\n    tokenized_example = tokenizer(first_sentence, second_sentence,padding=True, truncation=True,max_length=256)\n    tokenized_example['label'] = option_to_index[example['answer']]\n    return tokenized_example\ntokenized_train_ds = train_ds.map(preprocess, batched=False, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])","metadata":{"execution":{"iopub.status.busy":"2023-09-16T14:34:24.176559Z","iopub.execute_input":"2023-09-16T14:34:24.177561Z","iopub.status.idle":"2023-09-16T14:34:24.980259Z","shell.execute_reply.started":"2023-09-16T14:34:24.177522Z","shell.execute_reply":"2023-09-16T14:34:24.979071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Following datacollator (adapted from https://huggingface.co/docs/transformers/tasks/multiple_choice)\n# will dynamically pad our questions at batch-time so we don't have to make every question the length\n# of our longest question.\nfrom datasets import Dataset\nfrom dataclasses import dataclass\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\nfrom typing import Optional, Union\nimport torch\n\n@dataclass\nclass DataCollatorForMultipleChoice:\n    tokenizer: PreTrainedTokenizerBase\n    padding: Union[bool, str, PaddingStrategy] = 'max_length'\n    max_length: Optional[int] = 512\n    pad_to_multiple_of: Optional[int] = None\n    \n    def __call__(self, features):\n        label_name = \"label\" if 'label' in features[0].keys() else 'labels'\n        labels = [feature.pop(label_name) for feature in features]\n        batch_size = len(features)\n        num_choices = len(features[0]['input_ids'])\n        flattened_features = [\n            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n        ]\n        flattened_features = sum(flattened_features, [])\n        \n        batch = self.tokenizer.pad(\n            flattened_features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors='pt',\n        )\n        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n        return batch\n","metadata":{"execution":{"iopub.status.busy":"2023-09-16T14:34:58.517583Z","iopub.execute_input":"2023-09-16T14:34:58.518295Z","iopub.status.idle":"2023-09-16T14:34:58.530030Z","shell.execute_reply.started":"2023-09-16T14:34:58.518260Z","shell.execute_reply":"2023-09-16T14:34:58.528833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The arguments here are selected to run quickly; feel free to play with them.\nimport shutil\n#shutil.rmtree('finetuned_bert')\nmodel_dir = 'finetuned_bert'\ntraining_args = TrainingArguments(\n    output_dir=model_dir,\n    evaluation_strategy=\"epoch\",\n#    save_strategy=\"epoch\",\n#    load_best_model_at_end=True,\n    learning_rate=5e-5,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    num_train_epochs=15,\n    weight_decay=0.01,\n    report_to='none'\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T14:35:42.621621Z","iopub.execute_input":"2023-09-16T14:35:42.622021Z","iopub.status.idle":"2023-09-16T14:35:42.632609Z","shell.execute_reply.started":"2023-09-16T14:35:42.621989Z","shell.execute_reply":"2023-09-16T14:35:42.631717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generally it's a bad idea to validate on your training set, but because our training set\n# for this problem is so small we're going to train on all our data.\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train_ds,\n    eval_dataset=tokenized_train_ds,\n    tokenizer=tokenizer,\n    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T14:35:44.901557Z","iopub.execute_input":"2023-09-16T14:35:44.901937Z","iopub.status.idle":"2023-09-16T14:35:44.934120Z","shell.execute_reply.started":"2023-09-16T14:35:44.901904Z","shell.execute_reply":"2023-09-16T14:35:44.933201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n_ = gc.collect() # garbage collector..frees up memmory\nlibc.malloc_trim(0)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T14:35:46.321632Z","iopub.execute_input":"2023-09-16T14:35:46.322019Z","iopub.status.idle":"2023-09-16T14:35:46.757781Z","shell.execute_reply.started":"2023-09-16T14:35:46.321988Z","shell.execute_reply":"2023-09-16T14:35:46.756680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training should take about a minute\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-09-16T14:35:48.149714Z","iopub.execute_input":"2023-09-16T14:35:48.150815Z","iopub.status.idle":"2023-09-16T14:52:50.378707Z","shell.execute_reply.started":"2023-09-16T14:35:48.150772Z","shell.execute_reply":"2023-09-16T14:52:50.377775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The following function gets the indices of the highest scoring answers for each row\n# and converts them back to our answer format (A, B, C, D, E)\nimport numpy as np\ndef predictions_to_map_output(predictions):\n    sorted_answer_indices = np.argsort(-predictions)\n    top_answer_indices = sorted_answer_indices[:,:3] # Get the first three answers in each row\n    top_answers = np.vectorize(index_to_option.get)(top_answer_indices)\n    return np.apply_along_axis(lambda row: ' '.join(row), 1, top_answers)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T14:52:58.995461Z","iopub.execute_input":"2023-09-16T14:52:58.996239Z","iopub.status.idle":"2023-09-16T14:52:59.003139Z","shell.execute_reply.started":"2023-09-16T14:52:58.996189Z","shell.execute_reply":"2023-09-16T14:52:59.002220Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## INFERENCING ON TEST DF","metadata":{}},{"cell_type":"code","source":"del df_train\ndel context_df_train\ndel model_train_df\ndel tokenized_train_ds\n\n_ = gc.collect() # garbage collector..frees up memmory\nlibc.malloc_trim(0)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T14:53:36.191164Z","iopub.execute_input":"2023-09-16T14:53:36.191544Z","iopub.status.idle":"2023-09-16T14:53:36.581169Z","shell.execute_reply.started":"2023-09-16T14:53:36.191511Z","shell.execute_reply":"2023-09-16T14:53:36.580109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chunk_size = 1000\nchunk_overlap = 100\n\nr_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=chunk_size,\n    chunk_overlap=chunk_overlap\n)\n\nsplit_text =[]\nfor i in range(len(context_df_test)):\n    split_text.append ( r_splitter.split_text(context_df_test.loc[i,'text']))\ncontext_df_test['split'] = split_text","metadata":{"execution":{"iopub.status.busy":"2023-09-16T14:54:08.652885Z","iopub.execute_input":"2023-09-16T14:54:08.653280Z","iopub.status.idle":"2023-09-16T14:54:12.746508Z","shell.execute_reply.started":"2023-09-16T14:54:08.653249Z","shell.execute_reply":"2023-09-16T14:54:12.745502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = format_dataframe(df_test,context_df_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T14:54:14.532477Z","iopub.execute_input":"2023-09-16T14:54:14.532847Z","iopub.status.idle":"2023-09-16T14:54:56.816062Z","shell.execute_reply.started":"2023-09-16T14:54:14.532814Z","shell.execute_reply":"2023-09-16T14:54:56.815148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model_test[[\"prompt\", \"context\", \"A\", \"B\", \"C\", \"D\", \"E\"]].to_csv(\"./test_context.csv\", index=False)\n#test_df = pd.read_csv(\"test_context.csv\")\n#test_df.index = list(range(len(test_df)))\n#test_df['id'] = list(range(len(test_df)))\n#test_df['context'] = test_df['context'].apply(lambda x: str(x))\n#test_df[\"prompt\"] = test_df[\"context\"] + \" #### \" +  test_df[\"prompt\"]\ntest_df['answer'] = 'B'","metadata":{"execution":{"iopub.status.busy":"2023-09-16T14:55:01.874966Z","iopub.execute_input":"2023-09-16T14:55:01.875367Z","iopub.status.idle":"2023-09-16T14:55:01.880775Z","shell.execute_reply.started":"2023-09-16T14:55:01.875336Z","shell.execute_reply":"2023-09-16T14:55:01.879839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds = Dataset.from_pandas(test_df)\ntokenized_test_ds = test_ds.map(preprocess, batched=False, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])","metadata":{"execution":{"iopub.status.busy":"2023-09-16T14:55:03.507447Z","iopub.execute_input":"2023-09-16T14:55:03.508148Z","iopub.status.idle":"2023-09-16T14:55:04.522544Z","shell.execute_reply.started":"2023-09-16T14:55:03.508112Z","shell.execute_reply":"2023-09-16T14:55:04.521599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here we'll generate our \"real\" predictions on the test set\ntest_predictions = trainer.predict(tokenized_test_ds)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T14:55:05.480375Z","iopub.execute_input":"2023-09-16T14:55:05.480732Z","iopub.status.idle":"2023-09-16T14:55:23.763106Z","shell.execute_reply.started":"2023-09-16T14:55:05.480697Z","shell.execute_reply":"2023-09-16T14:55:23.762197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = test_df[['id']]\nsubmission_df['prediction'] = predictions_to_map_output(test_predictions.predictions)\n\nsubmission_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-16T14:55:24.785598Z","iopub.execute_input":"2023-09-16T14:55:24.786000Z","iopub.status.idle":"2023-09-16T14:55:24.816832Z","shell.execute_reply.started":"2023-09-16T14:55:24.785966Z","shell.execute_reply":"2023-09-16T14:55:24.815929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Once we write our submission file we're good to submit!\nsubmission_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T14:55:26.648968Z","iopub.execute_input":"2023-09-16T14:55:26.649329Z","iopub.status.idle":"2023-09-16T14:55:26.665169Z","shell.execute_reply.started":"2023-09-16T14:55:26.649300Z","shell.execute_reply":"2023-09-16T14:55:26.664196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}